{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#ashleys-qc-pipeline","title":"\ud83e\uddf9 Ashleys-QC Pipeline","text":"<p>This workflow uses Snakemake perform Quality Control analysis on Strand-Seq single-cell sequencing data. The starting point are single-cell FASTQ files from Strand-seq experiments and the final output produced is a folder with clean selected BAM files. The pipeline can identify automatically high-quality libraries through ML-based analysis tool ashleys-qc. Thus, the workflow goes through the following steps:</p> <ol> <li>FASTQ sequencing Quality Control through FastQC</li> <li>Mapping FASTQ against a reference genome throught BWA</li> <li>Sorting, Deduplicating and Indexing of BAM files through Samtools &amp; sambaba</li> <li>Generating features and use ashleys-qc model to identify high-quality cells</li> </ol>"},{"location":"#mosaicatcher","title":"\ud83e\udde9 MosaiCatcher","text":"<p>The starting point in MosaiCatcher are single-cell BAM files from Strand-seq experiments and the final output are SV predictions in a tabular format as well as in a graphical representation. To get to this point, the workflow goes through the following steps:</p> <ol> <li>Binning of sequencing reads in genomic windows of 200kb via mosaic</li> <li>Strand state detection</li> <li>[Optional] Normalization of coverage with respect to a reference sample</li> <li>Multi-variate segmentation of cells (mosaic)</li> <li>Haplotype resolution via StrandPhaseR</li> <li>Bayesian classification of segmentation to find SVs using MosaiClassifier</li> <li>Visualization of results using custom R plots</li> </ol> <p> </p> Figure 1: MosaiCatcher v2 schematic representation and visualizations examples. <p>MosaiCatcher v2 schematic representation and visualizations examples. (A) MosaiCatcher v2 pipeline schematic representation: On the left part in dimmed orange is represented ashleys-qc-pipeline, a switchable preprocessing optional module that allows to perform standard steps of mapping, sorting, and indexing FASTQ libraries, producing quality control plots and reports as well as identifying high-quality libraries. On the right uncolored part, the MosaiCatcher core part of the pipeline is still usable as a standalone by providing Stand-Seq aligned BAM files. Green boxes correspond to data-conditional dependent execution steps (Snakemake checkpoints) that allow more flexibility and reduce issues when executing the workflow. Orange box corresponds to the multi-step normalization module. Blue box corresponds to ArbiGent mode of execution that allows SV genotyping from arbitrary segmentation. Violet box corresponds to scNOVA SV function analysis mode of execution. Dashed boxes correspond to optional modules. *: Strand-seq QC plot is only produced here if ashleys-qc pipeline is not enabled (B) quality control Strand-seq karyotype visualization based on read counting according to a defined window (here 200 kb). A karyotype plot is available for each single-cell sequenced. Additional statistics are also presented in the upper part of the figure. These plots allow the users to perform QC to assess sequencing integrity. (C) SV call clustering heatmap and chromosome-wise visualizations. The clustering heatmap on the left generates a global SV distribution representation of the sample. Each SV call is represented as a column and each library as a row. Top row annotation corresponds to chromosomes and individual heatmap cell color corresponds to a specific type of haplotype-phased SV defined in the legend. The chromosome-wise representation on the right side of the subfigure summarizes, for a given chromosome (here chr10), the different layers of information computed during the pipeline: binned read counts, joint and cell segmentation, and phased SV calls. (D) Differential nucleosome occupancy heatmap representation: this heatmap computed with the scNOVA downstream module exemplifies gene-body-based differential nucleosome occupancy profiles (Jeong et al. 2023).</p>"},{"location":"#roadmap","title":"\ud83d\udcc6 Roadmap","text":""},{"location":"#technical-related-features","title":"Technical-related features","text":"<ul> <li> Zenodo automatic download of external files + indexes (1.2.1)</li> <li> Multiple samples in the parent folder (1.2.2)</li> <li> Automatic testing of BAM SM tag compared to sample folder name (1.2.3)</li> <li> On-error/success e-mail (1.3)</li> <li> HPC execution (slurm profile for the moment) (1.3)</li> <li> Full singularity image with preinstalled conda envs (1.5.1)</li> <li> Single BAM folder with side config file (1.6.1)</li> <li> (EMBL) GeneCore mode of execution: allow selection and execution directly by specifying genecore run folder (2022-11-02-H372MAFX5 for instance) (1.8.2)</li> <li> Version synchronisation between ashleys-qc-pipeline and mosaicatcher-pipeline (1.8.3)</li> <li> Report captions update (1.8.5)</li> <li> Clustering plot (heatmap) &amp; SV calls plot update (1.8.6)</li> <li> <code>ashleys_pipeline_only</code> parameter: using mosaicatcher-pipeline, trigger ashleys-qc-pipeline only and will stop after the generation of the counts, ashleys predictions &amp; plots to allow the user manual reviewing/selection of the cells to be processed (2.2.0)</li> <li> Target alternative execution ending: <code>breakpointr_only</code> parameter to stop the execution after breakpointR ; <code>whatshap_only</code> parameter to stop the execution after whatshap (2.3.3)</li> <li> Plotting options (enable/disable segmentation back colors)</li> </ul>"},{"location":"#bioinformatic-related-features","title":"Bioinformatic-related features","text":"<ul> <li> Self-handling of low-coverage cells (1.6.1)</li> <li> Upstream ashleys-qc-pipeline and FASTQ handle (1.6.1)</li> <li> Change of reference genome (currently only GRCh38) (1.7.0)</li> <li> Ploidy detection at the segment and the chromosome level: used to bypass StrandPhaseR if more than half of a chromosome is haploid (1.7.0)</li> <li> inpub_bam_legacy mode (bam/selected folders) (1.8.4)</li> <li> Blacklist regions files for T2T &amp; hg19 (1.8.5)</li> <li> ArbiGent integration: Strand-Seq based genotyper to study SV containly at least 500bp of uniquely mappable sequence (1.9.0)</li> <li> scNOVA integration: Strand-Seq Single-Cell Nucleosome Occupancy and genetic Variation Analysis (1.9.2)</li> <li> <code>multistep_normalisation</code> and <code>multistep_normalisation_for_SV_calling</code> parameters to replace GC analysis module (library size normalisation, GC correction, Variance Stabilising Transformation) (2.1.1)</li> <li> Strand-Seq processing based on mm10 assembly (2.1.2)</li> <li> UCSC ready to use file generation including counts &amp; SV calls (2.1.2)</li> <li> <code>blacklist_regions</code> parameter: (2.2.0)</li> <li> IGV ready to use XML session generation: (2.2.2)</li> <li> BreakpointR integration through <code>breakpointr</code> parameter (2.3.3)</li> <li> Pooled samples</li> </ul>"},{"location":"#small-issues-to-fix","title":"Small issues to fix","text":"<ul> <li> replace <code>input_bam_location</code> by <code>data_location</code> (harmonization with ashleys-qc-pipeline)</li> <li> List of commands available through list_commands parameter (1.8.6</li> <li> Move pysam / SM tag comparison script to snakemake rule (2.2.0)</li> <li> Reference properly reference genome in IGV session script generation (2.3.5</li> </ul>"},{"location":"#troubleshooting-current-limitations","title":"\ud83d\uded1 Troubleshooting &amp; Current limitations","text":"<ul> <li>Do not change the structure of your input folder after running the pipeline, first execution will build a config dataframe file (<code>OUTPUT_DIRECTORY/config/config.tsv</code>) that contains the list of cells and the associated paths</li> <li>Do not change the list of chromosomes after a first execution (i.e: first execution on <code>chr17</code>, second execution on all chromosomes)</li> </ul>"},{"location":"#authors-alphabetical-order","title":"\ud83d\udc82\u200d\u2642\ufe0f Authors (alphabetical order)","text":"<ul> <li>Ashraf Hufash</li> <li>Cosenza Marco</li> <li>Ebert Peter</li> <li>Ghareghani Maryam</li> <li>Grimes Karen</li> <li>Gros Christina</li> <li>H\u00f6ps Wolfram</li> <li>Jeong Hyobin</li> <li>Kinanen Venla</li> <li>Korbel Jan</li> <li>Marschall Tobias</li> <li>Meiers Sasha</li> <li>Porubsky David</li> <li>Rausch Tobias</li> <li>Sanders Ashley</li> <li>Van Vliet Alex</li> <li>Weber Thomas (maintainer and current developer)</li> </ul>"},{"location":"#citing-mosaicatcher","title":"Citing MosaiCatcher","text":"<p>When using MosaiCatcher for a publication, please cite the following article in your paper:</p> <p>MosaiCatcher v2 publication: Weber Thomas, Marco Raffaele Cosenza, and Jan Korbel. 2023. \u2018MosaiCatcher v2: A Single-Cell Structural Variations Detection and Analysis Reference Framework Based on Strand-Seq\u2019. Bioinformatics 39 (11): btad633. https://doi.org/10.1093/bioinformatics/btad633.</p>"},{"location":"#references","title":"\ud83d\udcd5 References","text":"<p>MosaiCatcher v2 publication: Weber Thomas, Marco Raffaele Cosenza, and Jan Korbel. 2023. \u2018MosaiCatcher v2: A Single-Cell Structural Variations Detection and Analysis Reference Framework Based on Strand-Seq\u2019. Bioinformatics 39 (11): btad633. https://doi.org/10.1093/bioinformatics/btad633</p> <p>Strand-seq publication: Falconer, E., Hills, M., Naumann, U. et al. DNA template strand sequencing of single-cells maps genomic rearrangements at high resolution. Nat Methods 9, 1107\u20131112 (2012). https://doi.org/10.1038/nmeth.2206</p> <p>scTRIP/MosaiCatcher original publication: Sanders, A.D., Meiers, S., Ghareghani, M. et al. Single-cell analysis of structural variations and complex rearrangements with tri-channel processing. Nat Biotechnol 38, 343\u2013354 (2020). https://doi.org/10.1038/s41587-019-0366-x</p> <p>ArbiGent publication: Porubsky, David, Wolfram H\u00f6ps, Hufsah Ashraf, PingHsun Hsieh, Bernardo Rodriguez-Martin, Feyza Yilmaz, Jana Ebler, et al. 2022. \u201cRecurrent Inversion Polymorphisms in Humans Associate with Genetic Instability and Genomic Disorders.\u201d Cell 185 (11): 1986-2005.e26. https://doi.org/10.1016/j.cell.2022.04.017</p> <p>scNOVA publication: Jeong, Hyobin, Karen Grimes, Kerstin K. Rauwolf, Peter-Martin Bruch, Tobias Rausch, Patrick Hasenfeld, Eva Benito, et al. 2022. \u201cFunctional Analysis of Structural Variants in Single Cells Using Strand-Seq.\u201d Nature Biotechnology, November, 1\u201313. https://doi.org/10.1038/s41587-022-01551-4</p>"},{"location":"installation/","title":"Installation &amp; Update","text":"<p>ashleys-qc-pipeline &amp; mosaicatcher-pipeline dependencies</p> <p>From 2.2.0, you don't need to clone both ashleys-qc-pipeline preprocessing module and mosaicatcher-pipeline. By using <code>ashleys_pipeline_only=True</code> combined with <code>ashleys_pipeline=True</code> in the configuration of MosaiCatcher, this will stop the execution after the generation of the files related to ashleys-qc-pipeline. This allow you to use a single pipeline, repository and container and limit the potential conflicts by processing the same folder (data_location) by different repositories and set of files (including the workflow/data/ref_genomes references files).</p>"},{"location":"installation/#system-requirements","title":"System requirements","text":"<p>This workflow is meant to be run in a Unix-based operating system when using the local execution profiles (tested on Ubuntu 18.04 &amp; CentOS 7).</p> <p>Minimum system requirements vary based on the use case. We highly recommend running it in a server environment with 32+GB RAM and 12+ cores.</p> <ul> <li>Conda install instructions</li> <li>Singularity install instructions</li> </ul> <p>Aside local execution, the pipeline can be run on a HPC cluster. The pipeline has been tested on SLURM-based clusters.</p>"},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#0-optional-install-apptainer","title":"0. [Optional] Install Apptainer","text":"<p>In order to run the pipeline, you can use the Apptainer containerization tool. This will allow you to run the pipeline in a controlled environment, with all the dependencies installed and ready to use.</p>"},{"location":"installation/#1-install-snakemake-through-conda-in-a-dedicated-env","title":"1. Install snakemake through conda in a dedicated env","text":"<p>Snakemake version compatibility</p> <p>The pipeline has been tested with Snakemake version 7+. Some tests are currently ongoing to ensure compatibility with Snakemake 8+.</p> <p>To install the required snakemake dependencies, you can use the following commands:</p> <pre><code>conda create -n snakemake-env \\\n    -c conda-forge -c bioconda snakemake==7.32.4\nconda activate snakemake-env\n</code></pre> <p>If you prefer to use a python virtual environment, you can use the following commands:</p> <pre><code>python3 -m venv snakemake-env\nsource snakemake-env/bin/activate\npip install snakemake==7.32.4\n</code></pre> <p>Note</p> <p>Instead of installing your own snakemake environment, you can load an existing snakemake module with the following command: </p> <pre><code>module load snakemake/7.32.4-foss-2022b\n</code></pre>"},{"location":"installation/#2-clone-the-repository-and-its-submodules","title":"2. Clone the repository and its submodules","text":"<pre><code># Clone the repository and its submodules\ngit clone --recurse-submodules https://github.com/friendsofstrandseq/mosaicatcher-pipeline.git &amp;&amp; cd mosaicatcher-pipeline\n\n# In each submodule, initialize and pull\ngit submodule update --init --remote --force --recursive\n</code></pre>"},{"location":"installation/#pipeline-update-procedure","title":"Pipeline update procedure","text":"<p>If you already use a previous version of mosaicatcher-pipeline, here is a short procedure to update it:</p> <ul> <li>First, update all origin/ refs to latest: <p><code>git fetch --all</code></p> <ul> <li>Jump to a new version &amp; pull code:</li> </ul> <p><code>git checkout &lt;VERSION&gt; &amp;&amp; git pull</code></p> <p>Then, to initiate or update git submodules:</p> <p><code>git submodule update --init --remote --force --recursive</code></p>"},{"location":"mosaic_count/","title":"Mosaic count plots examples","text":""},{"location":"mosaic_count/#mosaic-count-plots-examples_1","title":"Mosaic count plots examples","text":""},{"location":"mosaic_count/#high-quality-example","title":"High-quality example","text":"High-quality example High-quality example"},{"location":"mosaic_count/#low-quality-examples","title":"Low-quality examples","text":""},{"location":"mosaic_count/#under-brdu-incorporation","title":"Under-BrdU incorporation","text":"Under-BrdU incorporation little spikes = nascent DNA strand that were not degrade properly"},{"location":"mosaic_count/#over-brdu-incorporation","title":"Over-BrdU incorporation","text":"Over-BrdU incorporation BrdU degraded both nascent and template strands"},{"location":"mosaic_count/#background-contamination","title":"Background contamination","text":"Background contamination Noisy background reads make all chromosomes look WC"},{"location":"mosaic_count/#multiple-cell-sequenced-instead-of-single","title":"Multiple cell sequenced instead of single","text":"Multi cell (Positive control)"},{"location":"mosaic_count/#no-cell-sequenced","title":"No cell sequenced","text":"Zero cell (Negative control) Under-BrdU incorporation little spikes = nascent DNA strand that were not degrade properly"},{"location":"mosaic_count/#over-brdu-incorporation_1","title":"Over-BrdU incorporation","text":"Over-BrdU incorporation BrdU degraded both nascent and template strands"},{"location":"mosaic_count/#background-contamination_1","title":"Background contamination","text":"Background contamination Noisy background reads make all chromosomes look WC"},{"location":"mosaic_count/#multiple-cell-sequenced-instead-of-single_1","title":"Multiple cell sequenced instead of single","text":"Multi cell (Positive control)"},{"location":"mosaic_count/#no-cell-sequenced_1","title":"No cell sequenced","text":"Zero cell (Negative control)"},{"location":"outputs/","title":"Outputs","text":"<p>The files listed below will be created in the selected results directory (<code>output_location</code> parameter). All paths are relative to the top-level results directory.</p>"},{"location":"outputs/#plots-folder","title":"Plots folder","text":""},{"location":"outputs/#mosaic-count-reads-density-across-bins","title":"Mosaic count - reads density across bins","text":"<p>File path: <code>&lt;OUTPUT_FOLDER&gt;/&lt;SAMPLE&gt;/plots/counts/CountComplete.pdf</code></p> <p>Report category: <code>Mosaic counts</code></p> <p>The plots present in this file will allow you to visualize both global statistics at the sample level but also individual Strand-Seq karyotypes at the cell level.</p> <p>The first page of the pdf file shows the statistics of the analysis result, such as the distribution of total number of reads per cell, duplication rate, or excluded bins per chromosomes.</p> Global statistics - Sample level <p>Afterwards, every pages show the overview of binning count result of each of the single-cells as presented below. The depth of Crick reads are depicted in the green color in the right side, and the depth of Watson reads are depicted in the orange color in the left side of each chromosome lines. HMM automatically defines the WW/WC/CC status according the reads distribution (yellow background: WC, green background: CC, orange background: WW).</p> Strand-seq karyotype visualisation <p>Strand-seq karyotype visualisation based on reads counting according defined window (here 100kb). Additionnal statistics are also presented in the upper part of the figure.</p> <p>Examples of good-quality and low-quality cells are presented here.</p>"},{"location":"outputs/#sv-calls","title":"SV calls","text":"<p>File path: <code>&lt;OUTPUT_FOLDER&gt;/plots/&lt;SAMPLE&gt;/sv_calls/&lt;METHOD&gt;/&lt;CHR&gt;.pdf</code></p> <p>Report category: <code>SV Calls</code></p> <p>The SV calls plots allow to identify and analyse at the single-cell level, the SV detected in the pipeline.</p> <p>SV calls plot correspond to chromosome-wise plots summarizing all informations computed during the pipeline (count binning = orange/green core signal, grey/white background: mosaic segments, bottom green/yellow/orange line: W/C state, additional colors: SV groups).</p> <p>Each different file corresponds to a different chromosome and each file presents a different track for each library processed successfully during the pipeline.</p> <p>As two different filtering methods are currently set (stringent/lenient filtering, stringent: a SV must be present in more than 5% of the cells), two groups of files can be found:</p> <ul> <li>stringent_filterTRUE/chr.pdf <li>lenient_filterFALSE/chr.pdf <p>Description of the parameters used can be found in the parameters section here.</p> SV calls Here we can identify heterozygous deletions (blue) as well as heterozygous duplication (pink) on cells PE20453 and PE20458."},{"location":"outputs/#sv-consistency","title":"SV consistency","text":"<p>File path: <code>&lt;OUTPUT_FOLDER&gt;/&lt;SAMPLE&gt;/plots/sv_consistency/&lt;METHOD&gt;.consistency-barplot-&lt;byaf|bypos&gt;.pdf</code></p> <p>Report category: <code>SV Consistency</code></p> <p>SV consistency plots correspond to barplots representing SV events (rows) according their frequency across cells and their class (del, dup, inv, ...).</p> <p>These plots are complentary to the other outputs available and are presented either sorted by Variant Allele Fraction (VAF) or by position.</p> <p>Like SV calls and SV clustering plots, SV consistency are also presented regarding the two filtering methods (stringent/lenient filtering), two groups of files can be found:</p> <ul> <li>stringent_filterTRUE..pdf <li>lenient_filterFALSE..pdf <p>Here are some important points to better analyse these plots:</p> <ul> <li>If the VAF is close to 1, the SVs are expected to be germline variant</li> <li>If the VAF is below 1, the SVs are expected to be somatic variant</li> <li>If the SVs only detected by one cell, it can be rare SV event, or an SCE (sister chromatid exchange) event</li> <li>SCEs happen independently in each single cell, and unlike SVs, SCEs are not transmitted clonally to   daughter cells. Hence, changepoints resulting from SCEs are very unlikely to recur at the same position in &gt;1 cell of a sample</li> </ul> SV consistency (position sorted)"},{"location":"outputs/#sv-clustering","title":"SV clustering","text":"<p>File path: <code>&lt;OUTPUT_FOLDER&gt;/&lt;SAMPLE&gt;/plots/sv_clustering/&lt;CELL&gt;.pdf</code></p> <p>Report category: <code>SV Clustering</code></p> <p>SV clustering are complementary to previously presented plots, as the heatmap representation allow user to have a complete and global representation of the SVs at the sample level.</p> <p>Each file is composed of two plots:</p> <ul> <li>a clustering heatmap where cells (rows) were ordered automatically using Ward Hierarchical Agglomerative clustering based on the number of cells presenting each SV (columns). SV are horizontally sorted by genomic position (chr1..22, X, Y)</li> <li>a similar heatmap (same horizontal and vertical sorting) but highlighting both SV type (dup, del, inv, ...) and SV haplotype phasing (H1/H2/Hom)</li> </ul> <p>Two types of files are accesible from the user:</p> <ul> <li>simpleCalls_llr4_poppriorsTRUE_haplotagsFALSE_gtcutoff0.05_regfactor6_filterTRUE-.pdf <li>position: no scaling</li> <li>chromosome: chromosome-size scaled</li> <p>By using these heatmaps, the user can easily identify subclones based on the SV position and enrichment across cells, as presented below.</p> 1. SV clustering (SV density) 2. SV clustering (SV type)"},{"location":"outputs/#genome-browsing","title":"Genome browsing","text":"<p>File path: <code>&lt;OUTPUT_FOLDER&gt;/&lt;SAMPLE&gt;/plots/UCSC|IGV</code></p> <p>You can now also generates UCSC and IGV genome-browsing ready-to-be-used files when <code>genome_browsing_files_generation=True</code>. Here is an example below:</p> (A) UCSC and (B) IGV Genome browsing using files generated by MosaiCatcher"},{"location":"outputs/#ucsc-genome-browser","title":"UCSC genome browser","text":"<p>To visualise your data on the UCSC genome browser, go to the website, section <code>My Data/Custom Tracks</code>, and upload the .gz file generated by MosaiCatcher located in <code>&lt;OUTPUT_FOLDER&gt;/&lt;SAMPLE&gt;/plots/UCSC/&lt;SAMPLE&gt;.bedUCSC.gz</code>. Then click on the <code>submit</code> button, wait until the loading complete. You should see the list of tracks loaded.</p>"},{"location":"outputs/#igv","title":"IGV","text":"<p>MosaiCatcher generates an XML session ready to use by IGV. Thus, files in this XML session are referenced using relative path. It's thus important to either mount the disk where is present the data on your computer, or to copy the complete <code>&lt;OUTPUT_FOLDER&gt;/&lt;SAMPLE&gt;/plots/IGV/</code> on your computer. Once this is done, open the software, click on <code>File/Open Session</code> and load the XML file present in the IGV folder. Please note that the SV calls coloring appears only once you are displaying the data at the chromosome level, not at the genome level.</p>"},{"location":"outputs/#sctrip-multiplot-marco-cosenza","title":"scTRIP multiplot (Marco Cosenza)","text":"<p>From 2.2.2, it's now possible to use scTRIP multiplot inside MosaiCatcher. By enabling the option available in the configuration (<code>scTRIP_multiplot=True</code>), you will obtain for each chromosome of each cell able to be processed, a plot similar to below:</p> scTRIP multiplot (Watson/Crick, depth, haplotype phased and SV calls)"},{"location":"outputs/#statistics","title":"Statistics","text":"<p>File path: <code>&lt;OUTPUT_FOLDER&gt;/&lt;SAMPLE&gt;/stats/stats-merged.tsv</code></p> <p>Report category: <code>Stats</code></p> Col nb Col title Comment 1 callset mosaiclassifier SV callset filename 2 cell_count number of librairies processed 3 segments number of segments identified 4 total_sce total nb of sister-chromatid-exchange identified 5 avg_sce_per_cell average nb of sister-chromatid-exchange per cell identified 6 total_calls total nb of SV detected 7 unique_calls nb of unique SV detected 8 unique_calls_merged 9 complex_lengths_mb cumulative size of complex SV (Mb) detected 10 total_calls_complex total nb of complex SV detected 11 unique_calls_complex nb of unique complex SV detected 12 avg_sv_load_per_cell_mb average size of SV (Mb) detected per cell 13 avg_sv_load_per_cell_complex_mb average size of complex SV (Mb) detected per cell 14 calls_af0to10 nb of SV calls where SV sample allelic frequency is 0 &lt; x &lt; 10% 15 calls_af10to80 nb of SV calls where SV sample allelic frequency is 10 &lt; x &lt; 80% 16 calls_af80to100 nb of SV calls where SV sample allelic frequency is 80 &lt; x &lt; 100% 17 length_sum_af0to10_mb cumulative length of SV calls where SV sample allelic frequency is 0 &lt; x &lt; 10% 18 length_sum_af10to80_mb cumulative length of SV calls where SV sample allelic frequency is 10 &lt; x &lt; 80% 19 length_sum_af80to100_mb cumulative length of SV calls where SV sample allelic frequency is 80 &lt; x &lt; 100% 20 calls_af0to10_complex nb of complex SV calls where SV sample allelic frequency is 0 &lt; x &lt; 10% 21 calls_af10to80_complex nb of complex SV calls where SV sample allelic frequency is 10 &lt; x &lt; 80% 22 calls_af80to100_complex nb of complex SV calls where SV sample allelic frequency is 80 &lt; x &lt; 100% 23 length_sum_af0to10_complex_mb cumulative length of complex SV calls where SV sample allelic frequency is 0 &lt; x &lt; 10% 24 length_sum_af10to80_complex_mb cumulative length of complex SV calls where SV sample allelic frequency is 10 &lt; x &lt; 80% 25 length_sum_af80to100_complex_mb cumulative length of complex SV calls where SV sample allelic frequency is 80 &lt; x &lt; 100%"},{"location":"outputs/#raw-sv-calls-tab-seperated-file","title":"Raw SV calls (tab-seperated file)","text":"<p>File path: <code>&lt;OUTPUT_FOLDER&gt;/&lt;SAMPLE&gt;/mosaiclassifier/sv_calls/&lt;method&gt;.pdf</code></p> <p>Columns:</p> Col nb Col title Comment 1 chrom chromosome 2 start chromosome start 3 end chromosome end 4 sample sample name 5 cell cell name 6 class WC/WW/CC/CW 7 scalar Corresponding scalar to haplotype class 8 num_bins number of bins overlapping SV 9 sv_call_name SV type corresponding to the highest likelihood ratio 10 sv_call_haplotype Scalar corresponding to haplotype class 11 sv_call_name_2nd SV type corresponding to the 2nd highest likelihood ratio 12 sv_call_haplotype_2nd Scalar corresponding to the haplotype class of the 2nd highest likelihood ratio 13 llr_to_ref Highest Likelihood ratio value 14 llr_to_2nd 2nd Highest Likelihood ratio value 15 af SV allelic frequency"},{"location":"outputs/#mosaic-count-plots-examples","title":"Mosaic count plots examples","text":""},{"location":"outputs/#high-quality-example","title":"High-quality example","text":"High-quality example High-quality example"},{"location":"outputs/#low-quality-examples","title":"Low-quality examples","text":""},{"location":"outputs/#under-brdu-incorporation","title":"Under-BrdU incorporation","text":"Under-BrdU incorporation little spikes = nascent DNA strand that were not degrade properly"},{"location":"outputs/#over-brdu-incorporation","title":"Over-BrdU incorporation","text":"Over-BrdU incorporation BrdU degraded both nascent and template strands"},{"location":"outputs/#background-contamination","title":"Background contamination","text":"Background contamination Noisy background reads make all chromosomes look WC"},{"location":"outputs/#multiple-cell-sequenced-instead-of-single","title":"Multiple cell sequenced instead of single","text":"Multi cell (Positive control)"},{"location":"outputs/#no-cell-sequenced","title":"No cell sequenced","text":"Zero cell (Negative control) Under-BrdU incorporation little spikes = nascent DNA strand that were not degrade properly"},{"location":"outputs/#over-brdu-incorporation_1","title":"Over-BrdU incorporation","text":"Over-BrdU incorporation BrdU degraded both nascent and template strands"},{"location":"outputs/#background-contamination_1","title":"Background contamination","text":"Background contamination Noisy background reads make all chromosomes look WC"},{"location":"outputs/#multiple-cell-sequenced-instead-of-single_1","title":"Multiple cell sequenced instead of single","text":"Multi cell (Positive control)"},{"location":"outputs/#no-cell-sequenced_1","title":"No cell sequenced","text":"Zero cell (Negative control)"},{"location":"parameters/","title":"Parameters","text":""},{"location":"parameters/#mosaicatcher-arguments","title":"MosaiCatcher arguments","text":"<p>How to pass configuration arguments?</p> <p>All these arguments can be specified in two ways:</p> <ol> <li>In the config/config.yaml file, by replacing existing values</li> <li>Using the <code>--config</code> snakemake argument (<code>--config</code> must be called only one time with all the arguments behind it, e.g: <code>--config input_bam_location=&lt;INPUT&gt; output_location=&lt;OUTPUT&gt; email=&lt;EMAIL&gt;</code>)</li> </ol>"},{"location":"parameters/#general-parameters","title":"General parameters","text":"Parameter Comment Default Example <code>email</code> Email address for completion summary None None"},{"location":"parameters/#data-location-inputoutput-options","title":"Data location &amp; Input/output options","text":"Parameter Comment Parameter type Default <code>data_location</code> Path to parent folder containing samples String .tests/data_CHR17/ <code>samples_to_process</code> If multiple plates in the data_location parent folder, specify one or a comma-sep list of samples None \"[SampleA,SampleB]\" <code>publishdir</code> Path to backup location where important data is copied String"},{"location":"parameters/#ashleys-qc-upstream-pipeline","title":"Ashleys-QC upstream pipeline","text":"Parameter Comment Parameter type Default <code>input_bam_legacy</code> Mutualy exclusive with ashleys_pipeline. Will use <code>selected</code> folder to identify high-quality libraries to process Boolean False <code>ashleys_pipeline</code> Allow to load and use ashleys-qc-pipeline snakemake preprocessing module and to start from FASTQ inputs Boolean False <code>ashleys_threshold</code> Threshold for Ashleys-qc binary classification Float 0.5 <code>bypass_ashleys</code> Set all cells as high-quality (labels to 1) False <code>MultiQC</code> Enable or disable MultiQC analysis (includes FastQC, samtools flagstats &amp; idxstats) Boolean False <code>hand_selection</code> Enable or disable hand selection through the Jupyter Notebook Boolean False <code>split_qc_plot</code> Enable or disable the split of QC plot into individual pages plots Boolean False <code>paired_end</code> Enable or disable the use of paired-end data Boolean False"},{"location":"parameters/#reference-data-chromosomes","title":"Reference data &amp; Chromosomes","text":"Parameter Comment Default (options) <code>reference</code> Reference genome hg38 (hg19, T2T, mm10) <code>chromosomes</code> List of chromosomes to be processed Human: chr[1..22,X,Y], Mouse: chr[1..20,X,Y] <code>chromosomes_to_exclude</code> List of chromosomes to exclude []"},{"location":"parameters/#counts-configuration","title":"Counts configuration","text":"Parameter Comment Default <code>window</code> Window size used for binning by mosaic count (Can be of high importance regarding library coverage) 100000 <code>blacklist_regions</code> Enable/Disable blacklisting True"},{"location":"parameters/#optional-modules","title":"Optional modules","text":"Parameter Comment Default <code>multistep_normalisation</code> Allow to perform multistep normalisation including GC correction for visualization (Marco Cosenza). False <code>breakpointR</code> Enable breakpointR module to compute breakpoints on Strand-Seq data (David Porubsky). False"},{"location":"parameters/#targeted-execution","title":"Targeted execution","text":"Parameter Comment Default <code>ashleys_pipeline_only</code> Stop the execution after ashleys-qc-pipeline submodule. Requires <code>ashleys_pipeline</code> to be True Boolean <code>breakpointR_only</code> Stop the execution after breakpointR submodule. Requires <code>breakpointR</code> to be True False <code>whatshap_only</code> Stop the execution after whatshap submodule (haplotagging of BAM files). False"},{"location":"parameters/#sv-calling-parameters","title":"SV calling parameters","text":"Parameter Comment Default <code>multistep_normalisation_for_SV_calling</code> Allow to use multistep normalisation count file during SV calling (Marco Cosenza). False <code>hgsvc_based_normalized_counts</code> Use HGSVC based normalisation . True"},{"location":"parameters/#sv-calling-algorithm-processing-options","title":"SV calling algorithm processing options","text":"Parameter Comment Default <code>min_diff_jointseg</code> Minimum difference in error term to include another breakpoint in the joint segmentation (default=0.5) 0.1 <code>min_diff_singleseg</code> Minimum difference in error term to include another breakpoint in the single-cell segmentation (default=1) 0.5 <code>additional_sce_cutoff</code> Minimum gain in mismatch distance needed to add an additional SCE 20000000 <code>sce_min_distance</code> Minimum distance of an SCE to a break in the joint segmentation 500000 <code>llr</code> Likelihood ratio used to detect SV calls 4"},{"location":"parameters/#downstream-analysis","title":"Downstream analysis","text":"Parameter Comment Default <code>arbigent</code> Enable ArbiGent mode of execution to genotype SV based on arbitrary segments False <code>arbigent_bed_file</code> Allow to specify custom ArbiGent BED file \"\" <code>scNOVA</code> Enable scNOVA mode of execution to compute Nucleosome Occupancy (NO) of detected SV False <code>scTRIP_multiplot</code> Enable scTRIP multiplot generation for all chromosomes of all cells False"},{"location":"parameters/#embl-specific-options","title":"EMBL specific options","text":"Parameter Comment Default <code>genecore</code> Enable/disable genecore mode to give as input the genecore shared folder in /g/korbel/shared/genecore False <code>genecore_date_folder</code> Specify folder to be processed <code>genecore_prefix</code> Specify genecore prefix folder /g/korbel/STOCKS/Data/Assay/sequencing/2023 <code>genecore_regex_elements</code> Specify genecore regex element to be used to distinguish sample from well number PE20 <p>If <code>genecore</code> and <code>genecore_date_folder</code> are correctly specified, each plate will be processed independently by creating a specific folder in the <code>data_location</code> folder.</p>"},{"location":"parameters/#execution-profile","title":"Execution profile","text":"<p>Location: workflow/snakemake_profiles/</p> Parameter Comment Conda Singularity HPC Local local/conda / X X local/conda_singularity / X X X HPC/slurm_generic (to modify) / X X HPC/slurm_EMBL (optimised for EMBL HPC) / X X"},{"location":"parameters/#snakemake-arguments","title":"Snakemake arguments","text":"<p>Here are presented some essential snakemake options that could help you.</p> <pre><code>--cores, -c\n</code></pre> <p>Use at most N CPU cores/jobs in parallel. If N is omitted or \u2018all\u2019, the limit is set to the number of available CPU cores. In case of cluster/cloud execution, this argument sets the number of total cores used over all jobs (made available to rules via workflow.cores).</p> <pre><code>--printshellcmds, -p\n</code></pre> <p>Recommended to print out the shell commands that will be executed.</p> <pre><code>--use-conda\n</code></pre> <p>If defined in the rule, run job in a conda environment. If this flag is not set, the conda directive is ignored and use the current environment (and path system) to execute the command.</p> <pre><code>--conda-frontend [mamba|conda]\n</code></pre> <p>Choose the conda frontend for installing environments. Mamba is much faster and highly recommended but could not be installed by default on your system. Default: \u201cconda\u201d</p> <pre><code>--use-singularity\n</code></pre> <p>If defined in the rule, run job within a singularity container. If this flag is not set, the singularity directive is ignored.</p> <pre><code>--singularity-args \"-B /mounting_point:/mounting_point\"\n</code></pre> <p>Pass additional args to singularity. <code>-B</code> stands for binding point between the host and the container.</p> <pre><code>--dryrun, -n\n</code></pre> <p>Do not execute anything, and display what would be done. If you have a very large workflow, use \u2013dry-run \u2013quiet to just print a summary of the DAG of jobs.</p> <pre><code>--rerun-incomplete, --ri\n</code></pre> <p>Re-run all jobs the output of which is recognized as incomplete.</p> <pre><code>--keep-going, -k\n</code></pre> <p>Go on with independent jobs if a job fails.</p> <pre><code>-T, --retries, --restart-times\n</code></pre> <p>Number of times to restart failing jobs (defaults to 0).</p> <pre><code>--forceall, -F\n</code></pre> <p>Force the execution of the selected (or the first) rule and all rules it is dependent on regardless of already created output.</p> <p>\u2139\ufe0f Note</p> <p>Currently, the binding command needs to correspond to the mounting point of your system (i.e: \"/tmp:/tmp\"). On seneca for example (EMBL), use <code>\"/g:/g\"</code> if you are working on <code>/g/korbel[2]</code> or <code>\"/scratch:/scratch\"</code> if you plan to work on <code>scratch</code>.</p> <p>Obviously, all other snakemake CLI options can also be used.</p>"},{"location":"publication/","title":"Reproduce publication results","text":""},{"location":"publication/#publication-full-example","title":"Publication full example","text":"<p>If you wish to reproduce results generated in the MosaiCatcher v2 publication, you can follow these different steps:</p>"},{"location":"publication/#download-and-prepare-data","title":"Download and prepare data","text":"<p>As the complete data was above 50GB, data was stored into 2 separated zenodo repositories: https://zenodo.org/record/7696695 &amp; https://zenodo.org/record/7697329. You can then first create a directory in a location with at least 300GB of free space, download the 3 tar.gz and extract them:</p> <pre><code># Specify your location\nYOUR_LOCATION=DEFINE_YOUR_PATH_HERE\n# Create directory\nmkdir -p $YOUR_LOCATION/mosaicatcher_publication_data/COMPRESSED\n# Download tar.gz parts\nwget https://zenodo.org/record/7696695/files/MOSAICATCHER_DATA_PAPER.tar.gz.part1?download=1 -P $YOUR_LOCATION/mosaicatcher_publication_data/COMPRESSED\nwget https://zenodo.org/record/7697329/files/MOSAICATCHER_DATA_PAPER.tar.gz.part2?download=1 -P $YOUR_LOCATION/mosaicatcher_publication_data/COMPRESSED\nwget https://zenodo.org/record/7697329/files/MOSAICATCHER_DATA_PAPER.tar.gz.part3?download=1 -P $YOUR_LOCATION/mosaicatcher_publication_data/COMPRESSED\n# Extract \nfor f in *part*; do tar -xvf \"$f\" -C $YOUR_LOCATION/mosaicatcher_publication_data; done\n</code></pre> <p>You can then verify checksum to make sure files were downloaded properly.</p> <pre><code># Verify checksum\nfor f in C7 RPE-BM510 RPE1-WT LCL; do\n    cd $YOUR_LOCATION/\"$f\"/fastq &amp;&amp; md5sum -c *.md5 &amp;&amp; cd $YOUR_LOCATION\ndone\n</code></pre> <p>If all the checks were successfull, you can now remove this directory before running the pipeline.</p> <pre><code>rm -r $YOUR_LOCATION/mosaicatcher_publication_data/COMPRESSED\n</code></pre>"},{"location":"publication/#run-the-pipeline","title":"Run the pipeline","text":"<p>A generic slurm profile is provided in the the following directory: mosaicatcher-pipeline/workflow/snakemake_profiles/HPC/slurm_generic. Feel free to edit, create another profile specify to your cluster institute. </p> <pre><code>snakemake \\\n    --config \\\n    data_location=$YOUR_LOCATION/mosaicatcher_publication_data \\\n    reference=T2T \\\n    split_qc_plot=True \\\n    ashleys_pipeline=True \\\n    multistep_normalisation=True \\\n    multistep_normalisation_for_SV_calling=False hgsvc_based_normalized_counts=True \\\n    --profile workflow/snakemake_profiles/HPC/slurm_generic/\n</code></pre>"},{"location":"publication/#generate-the-report","title":"Generate the report","text":"<pre><code>snakemake \\\n    --config \\\n    data_location=$YOUR_LOCATION/mosaicatcher_publication_data \\\n    reference=T2T \\\n    split_qc_plot=True \\\n    ashleys_pipeline=True \\\n    multistep_normalisation=True \\\n    multistep_normalisation_for_SV_calling=False hgsvc_based_normalized_counts=True \\\n    --profile workflow/snakemake_profiles/HPC/slurm_generic/\n    --report $YOUR_LOCATION/MosaiCatcher_V2_publication_data.report.zip --report-stylesheet workflow/report/custom-stylesheet.css\n</code></pre> <p>You can then extract the zip archive and open the <code>report.html</code> file to access the HTML report.</p>"},{"location":"usage/","title":"Usage","text":"<p>Note</p> <ul> <li>Please be careful of your conda/mamba setup, if you applied specific constraints/modifications to your system, this could lead to some versions discrepancies.</li> <li>mamba is usually preferred but might not be installed by default on a shared cluster environment</li> <li>You will need to verify that this conda environment is activated and provide the right snakemake before each execution (<code>which snakemake</code> command)</li> </ul>"},{"location":"usage/#get-started","title":"Get Started","text":"<p>Note for \ud83c\uddea\ud83c\uddfa EMBL users</p> <p>Use the following command for singularity-args parameter: <code>--singularity-args \"-B /g,/scratch\"</code></p>"},{"location":"usage/#run-pipeline-on-example-data","title":"Run pipeline on example data","text":"<p>Run on example data on only one small chromosome (<code>&lt;disk&gt;</code> must be replaced by your disk letter/name, <code>/g</code> or <code>/scratch</code> at EMBL for example)</p> <pre><code>snakemake \\\n    --cores 6 \\\n    --configfile .tests/config/simple_config.yaml \\\n    --profile workflow/snakemake_profiles/mosaicatcher-pipeline/v7/local/conda_singularity/ \\\n    --singularity-args \"-B /&lt;disk&gt;:/&lt;disk&gt;\"\n</code></pre> <p>Then, generate a report on example data.</p> <pre><code>snakemake \\\n    --cores 1 \\\n    --configfile .tests/config/simple_config.yaml \\\n    --profile workflow/snakemake_profiles/mosaicatcher-pipeline/v7/local/conda_singularity/ \\\n    --singularity-args \"-B /disk:/disk\" \\\n    --report report.zip \\\n    --report-stylesheet workflow/report/custom-stylesheet.css\n</code></pre>"},{"location":"usage/#run-on-your-own-data-locally","title":"Run on your own data locally","text":"<ol> <li>Run your own analysis locally (<code>&lt;disk&gt;</code> must be replaced by your disk letter/name, <code>/g</code> or <code>/scratch</code> at EMBL for example)</li> </ol> <pre><code>snakemake \\\n    --cores &lt;N&gt; \\\n    --config \\\n        data_location=&lt;PATH&gt; \\\n    --profile workflow/snakemake_profiles/mosaicatcher-pipeline/v7/local/conda_singularity/ \\\n    --singularity-args \"-B /&lt;disk&gt;:/&lt;disk&gt;\"\n</code></pre>"},{"location":"usage/#detailed-instructions","title":"Detailed instructions","text":""},{"location":"usage/#mosaicatcher-full-execution-with-ashleys-qc-preprocessing-module-enabled","title":"MosaiCatcher full execution (with ashleys-qc preprocessing module enabled)","text":""},{"location":"usage/#directory-structure","title":"Directory structure","text":"<p>The ashleys-qc-pipeline takes as input Strand-Seq fastq file following the structure below:</p> <pre><code>Parent_folder\n|-- Sample_1\n|   `-- fastq\n|       |-- Cell_01.1.fastq.gz\n|       |-- Cell_01.2.fastq.gz\n|       |-- Cell_02.1.fastq.gz\n|       |-- Cell_02.2.fastq.gz\n|       |-- Cell_03.1.fastq.gz\n|       |-- Cell_03.2.fastq.gz\n|       |-- Cell_04.1.fastq.gz\n|       `-- Cell_04.2.fastq.gz\n|\n`-- Sample_2\n    `-- fastq\n        |-- Cell_21.1.fastq.gz\n        |-- Cell_21.2.fastq.gz\n        |-- Cell_22.1.fastq.gz\n        |-- Cell_22.2.fastq.gz\n        |-- Cell_23.1.fastq.gz\n        |-- Cell_23.2.fastq.gz\n        |-- Cell_24.1.fastq.gz\n        `-- Cell_24.2.fastq.gz\n</code></pre> <p>Thus, in a <code>Parent_Folder</code>, create a subdirectory <code>Parent_Folder/sampleName/</code> for each <code>sample</code>. Each Strand-seq FASTQ files of this sample need to go into the <code>fastq</code> folder and respect the following syntax: <code>&lt;CELL&gt;.&lt;1|2&gt;.fastq.gz</code>, <code>1|2</code> corresponding to the pair identifier.</p> <p>Samples naming</p> <p>We advice to limit to the number of 2 the \"_\" characters in your file names (ashleys-qc tool will react weirdly with 3 or more \"_\" in the sample name)</p>"},{"location":"usage/#mosaicatcher-execution-without-preprocessing","title":"MosaiCatcher execution (without preprocessing)","text":""},{"location":"usage/#bam-input-requirements","title":"BAM input requirements","text":"<p>It is important to follow these rules for Strand-Seq single-cell BAM data</p> <ul> <li>BAM file name ending by suffix: <code>.sort.mdup.bam</code></li> <li>One BAM file per cell</li> <li>Sorted and indexed</li> <li>If BAM files are not indexed, please use a writable folder in order that the pipeline generate itself the index <code>.bai</code> files</li> <li>Timestamp of index files must be newer than of the BAM files</li> <li>Each BAM file must contain a read group (<code>@RG</code>) with a common sample name (<code>SM</code>), which must match the folder name (<code>sampleName</code> above)</li> </ul> <p>Filtering of low-quality cells impossible to process</p> <p>From Mosaicatcher version \u2265 1.6.1, a snakemake checkpoint rule was defined (filter_bad_cells) to automatically remove from the analysis cells flagged as low-quality.</p> <p>The ground state to define this status (usable/not usable) is determined from column pass1 (enough coverage to process the cell) in mosaic count info file (see example below). The value of pass1 is not static and can change according the window size used (the larger the window, the higher the number of reads to retrieve in a given bin).</p> <p>Cells flagged as low-quality cells are listed in the following TSV file: <code>&lt;OUTPUT&gt;/cell_selection/&lt;SAMPLE&gt;/labels.tsv</code>.</p>"},{"location":"usage/#directory-organisation-current-behavior","title":"Directory organisation - Current behavior","text":"<p>In its current flavour, MosaiCatcher requires that input data must be formatted the following way :</p> <pre><code>Parent_folder\n|-- Sample_1\n|   `-- bam\n|       |-- Cell_01.sort.mdup.bam\n|       |-- Cell_02.sort.mdup.bam\n|       |-- Cell_03.sort.mdup.bam\n|       `-- Cell_04.sort.mdup.bam\n|\n`-- Sample_2\n    `-- bam\n        |-- Cell_21.sort.mdup.bam\n        |-- Cell_22.sort.mdup.bam\n        |-- Cell_23.sort.mdup.bam\n        `-- Cell_24.sort.mdup.bam\n</code></pre> <p>In a <code>Parent_Folder</code>, create a subdirectory <code>Parent_Folder/sampleName/</code> for each <code>sample</code>. Your Strand-seq BAM files of this sample go into the following folder:</p> <ul> <li><code>bam</code> for the total set of BAM files</li> </ul> <p>Using the classic behavior, cells flagged as low-quality will only be determined based on coverage see Note here.</p>"},{"location":"usage/#directory-organisation-old-behavior","title":"Directory organisation - Old behavior","text":"<p>Previous version of MosaiCatcher (version \u2264 1.5) needed not only a <code>all</code> directory as described above, but also a <code>selected</code> folder (now renamed <code>bam</code>), presenting only high-quality selected libraries wished to be processed for the rest of the analysis.</p> <p>You can still use this behavior by enabling the config parameter either by the command line: <code>=True</code> or by modifying the corresponding entry in the config/config.yaml file.</p> <p>Thus, in a <code>Parent_Folder</code>, create a subdirectory <code>Parent_Folder/sampleName/</code> for each <code>sample</code>. Your Strand-seq BAM files of this sample go into the following folder:</p> <ul> <li><code>all</code> for the total set of BAM files</li> <li><code>selected</code> for the subset of BAM files that were identified as high-quality (either by copy or symlink)</li> </ul> <p>Your <code>&lt;INPUT&gt;</code> directory should look like this:</p> <pre><code>Parent_folder\n|-- Sample_1\n|   |-- bam\n|   |   |-- Cell_01.sort.mdup.bam\n|   |   |-- Cell_02.sort.mdup.bam\n|   |   |-- Cell_03.sort.mdup.bam\n|   |   `-- Cell_04.sort.mdup.bam\n|   `-- selected\n|       |-- Cell_01.sort.mdup.bam\n|       `-- Cell_04.sort.mdup.bam\n|\n`-- Sample_2\n    |-- bam\n    |   |-- Cell_01.sort.mdup.bam\n    |   |-- Cell_02.sort.mdup.bam\n    |   |-- Cell_03.sort.mdup.bam\n    |   `-- Cell_04.sort.mdup.bam\n    `-- selected\n        |-- Cell_03.sort.mdup.bam\n        `-- Cell_04.sort.mdup.bam\n</code></pre> <p>Using the <code>input_bam_legacy</code> parameter, cells flagged as low-quality will be determined both based on their presence in the <code>selected</code> folder presented above and on coverage see Note here.</p> <p>Warning</p> <ul> <li>Using the <code>input_bam_legacy</code> parameter, only intersection between cells present in the selected folder and with enough coverage will be kept. Example: if a library is present in the selected folder but present a low coverage see Note here, this will not be processed.</li> <li><code>ashleys_pipeline=True</code> and <code>input_bam_legacy=True</code> are mutually exclusive</li> <li>We advice to limit to the number of 2 the \"\" characters in your file names (Ashleys-qc ML tool will react weidly with more than 3 \"\")</li> </ul> <p>The <code>--profile</code> argument will define whether you want to execute the workflow locally on your laptop/desktop (<code>workflow/snakemake_profiles/mosaicatcher-pipeline/v7/local/conda_singularity//</code>), or if you want to run it on HPC. A generic slurm profile is available (<code>workflow/snakemake_profiles/mosaicatcher-pipeline/v7/HPC/slurm_generic/</code>).</p>"},{"location":"usage/#local-execution-not-hpc-or-cloud","title":"Local execution (not HPC or cloud):","text":"<pre><code>snakemake \\\n    --cores &lt;N&gt; \\\n    --config \\\n        data_location=&lt;DATA_FOLDER&gt; \\\n    --profile workflow/snakemake_profiles/mosaicatcher-pipeline/v7/local/conda_singularity/ \\\n    --singularity-args \"-B /&lt;disk&gt;:/&lt;disk&gt;\"\n</code></pre>"},{"location":"usage/#hpc-execution-slurm","title":"HPC execution (SLURM)","text":"<p>HPC execution (require first that you modify the workflow/snakemake_profiles/mosaicatcher-pipeline/HPC/slurm_generic/config.yaml for your own cluster)</p> <pre><code>snakemake \\\n    --config \\\n        data_location=&lt;DATA_FOLDER&gt; \\\n    --profile workflow/snakemake_profiles/mosaicatcher-pipeline/v7/HPC/slurm_generic/ \\\n    --singularity-args \"-B /&lt;disk&gt;:/&lt;disk&gt;\"\n</code></pre> <p>Current strategy to solve HPC job Out-of-Memory (OOM)</p> <p>Workflow HPC execution usually needs to deal with out of memory (OOM) errors, out of disk space, abnormal paths or missing parameters for the scheduler. To deal with OOM, we are currently using snakemake restart feature that allows to automatically double allocated memory to the job at each attempt (limited to 6 for the moment). Then, if a job fails to run with the default 1GB of memory allocated, it will be automatically restarted tith 2GB at the 2nd attempt, 4GB at the 3rd, etc.</p> <p>EMBL HPC execution</p> <p>In the EMBL HPC execution profile, disks binding points already set in the snakemake profile <code>--profile workflow/snakemake_profiles/mosaicatcher-pipeline/v7/HPC/slurm_EMBL/</code></p>"},{"location":"usage/#generate-report-optional","title":"Generate report (optional)","text":"<p>Optionally, it's also possible to generate a static interactive HTML report to explore the different plots produced by MosaiCatcher, using the same command as before and just adding at the end the following: <code>--report &lt;report&gt;.zip --report-stylesheet workflow/report/custom-stylesheet.css</code>, which correspond to the following complete command:</p> <pre><code>snakemake \\\n    --config \\\n        data_location=&lt;INPUT_FOLDER&gt; \\\n    --singularity-args \"-B /&lt;mounting_point&gt;:/&lt;mounting_point&gt;\" \\\n    --profile workflow/snakemake_profiles/slurm_generic/ \\\n    --report &lt;report&gt;.zip --report-stylesheet workflow/report/custom-stylesheet.css\n</code></pre> <p>Note</p> <p>The zip file produced can be heavy (~1GB for 24 HGSVC samples ; 2000 cells) if multiple samples are processed in parallel in the same output folder.</p>"},{"location":"usage/#modes-of-execution","title":"Modes of execution","text":""},{"location":"usage/#arbigent-mode-of-execution","title":"ArbiGent mode of execution","text":"<p>From 1.9.0, it's now possible to run MosaiCatcher in order to genotype a given list of positions specified in a bed file. To do so, <code>arbigent</code> config parameter need to be set to <code>True</code>. Thus, an alternative branch of the pipeline will be executed instead of the classic one, targetting results produced by ArbiGent.</p> <pre><code>snakemake \\\n    --cores &lt;N&gt; \\\n    --config \\\n        data_location=&lt;INPUT_DATA_FOLDER&gt; \\\n        arbigent=True \\\n    --profile workflow/snakemake_profiles/local/conda_singularity/\n</code></pre> <p>A generic BED file is provided in <code>workflow/data/arbigent/manual_segmentation.bed</code>. A custom BED file can be specified by modifying <code>arbigent_bed_file</code> config parameter to the path of your choice.</p> <p>Note</p> <p>If you modify the chromosome list to be processed (remove chrX &amp; chrY for instance), a dedicated rule will extract only from the BED file, the rows matching the list of chromosomes to be analysed.</p>"},{"location":"usage/#scnova-mode-of-execution","title":"scNOVA mode of execution","text":"<p>Warning</p> <p>Following Anaconda licensing issues with EMBL, scNOVA can only be executed using profiles based on singularity where the conda environment were already built.</p> <p>From 1.9.2, it's now possible to run scNOVA directly from MosaiCatcher in order to determine the nucleosome occupancy associated to the SV calls provided during MosaiCatcher execution.</p> <p>To do so, mosaicatcher must be executed during a first step in order to generate the SV calls and associated plots/tables required to determine subclonality.</p> <p>Once the subclonality was determined, a table respecting the following template must be provided at this path <code>&lt;DATA_LOCATION&gt;/&lt;SAMPLE&gt;/scNOVA_input_user/input_subclonality.txt</code> where the different clones are named <code>clone&lt;N&gt;</code>:</p> Filename Subclonality TALL3x1_DEA5_PE20406 clone2 TALL3x1_DEA5_PE20414 clone2 TALL3x1_DEA5_PE20415 clone1 TALL3x1_DEA5_PE20416 clone1 TALL3x1_DEA5_PE20417 clone1 TALL3x1_DEA5_PE20418 clone1 TALL3x1_DEA5_PE20419 clone1 TALL3x1_DEA5_PE20421 clone1 TALL3x1_DEA5_PE20422 clone1 <p>Once done, you can run the exact same command as previously and set <code>scNOVA</code> config parameter to <code>True</code> like the following:</p> <pre><code>snakemake \\\n    --cores &lt;N&gt; \\\n    --config \\\n        data_location=&lt;INPUT_DATA_FOLDER&gt; \\\n        scNOVA=True \\\n    --profile workflow/snakemake_profiles/local/conda_singularity/\n</code></pre>"},{"location":"usage/#multistep-normalisation","title":"Multistep normalisation","text":"<p>From 2.1.0, it's now possible to use multistep normalisation (library size normalisation, GC correction, Variance Stabilising Transformation) resulting counts file, not only for visualisation purpose, but also to rely on it during the SV calling framework. To do so</p> <pre><code>snakemake \\\n    --cores &lt;N&gt; \\\n    --config \\\n        data_location=&lt;INPUT_DATA_FOLDER&gt; \\\n        multistep_normalisation=True \\\n        multistep_normalisation_for_SV_calling=True \\\n    --profile workflow/snakemake_profiles/local/conda_singularity/\n</code></pre>"},{"location":"usage/#sctrip-multiplot","title":"scTRIP multiplot","text":"<p>From 2.2.2, scTRIP multiplot (from Marco Cosenza) is now compatible with MosaiCatcher. The single requirement is to clone scTRIP multiplot repository (please reach out Marco if you want to access the repository, currently private) inside <code>workflow/scripts/plotting/scTRIP_multiplot</code>.</p> <p>By default, scTRIP multiplot is set to false, to enable it, please update the <code>config/config.yaml</code> file or use <code>scTRIP_multiplot=True</code> in the config section of the command line. An example of a scTRIP multiplot is available here</p> <p>Warning</p> <p><code>multistep_normalisation_for_SV_calling</code> is mutually exclusive with <code>hgsvc_based_normalisation</code></p>"}]}